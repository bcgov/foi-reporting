{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a25c33bf-2647-43f1-b2f4-a38cd7871610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import uuid\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "\n",
    "def append_with_incrementing_id(\n",
    "    new_df: DataFrame,\n",
    "    table_name: str,\n",
    "    id_column: str = \"id\",\n",
    "    order_by_column: str = None,\n",
    "    database: str = \"default\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends new_df to a Delta table with string auto-increment IDs like '1', '2', '3', ...\n",
    "\n",
    "    Parameters:\n",
    "    - new_df: The new data to insert.\n",
    "    - table_name: Target Delta table name (must exist).\n",
    "    - id_column: Name of the ID column (default: 'id').\n",
    "    - order_by_column: Optional: Column to use for ordering (for deterministic IDs).\n",
    "    - database: Databricks database containing the table.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    full_table_name = f\"{database}.{table_name}\"\n",
    "\n",
    "    if not spark._jsparkSession.catalog().tableExists(full_table_name):\n",
    "        raise Exception(f\"Table {full_table_name} does not exist. Please create it first.\")\n",
    "\n",
    "    existing_df = spark.table(full_table_name)\n",
    "\n",
    "    if id_column in existing_df.columns:\n",
    "        numeric_part_expr = F.regexp_extract(F.col(id_column), r\"(\\d+)$\", 1).cast(\"long\")\n",
    "        max_id_row = existing_df.select(F.max(numeric_part_expr)).collect()[0][0]\n",
    "        max_id = max_id_row if max_id_row is not None else 0\n",
    "    else:\n",
    "        max_id = 0\n",
    "\n",
    "    if order_by_column and order_by_column in new_df.columns:\n",
    "        windowSpec = Window.orderBy(F.col(order_by_column))\n",
    "    else:\n",
    "        windowSpec = Window.orderBy(F.lit(1))\n",
    "\n",
    "    new_df_with_number = new_df.withColumn(\n",
    "        \"__rownum\", F.row_number().over(windowSpec) + max_id\n",
    "    )\n",
    "\n",
    "    new_df_with_id = new_df_with_number.withColumn(\n",
    "        id_column, F.col(\"__rownum\")\n",
    "    ).drop(\"__rownum\")\n",
    "\n",
    "    cols = new_df_with_id.columns\n",
    "    ordered_cols = [id_column] + [c for c in cols if c != id_column]\n",
    "    new_df_with_id = new_df_with_id.select(ordered_cols)\n",
    "\n",
    "    # new_df_with_id.write.mode(\"append\").saveAsTable(full_table_name)\n",
    "\n",
    "    new_df_with_id.printSchema()\n",
    "    \n",
    "    new_df_with_id.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"false\").insertInto(full_table_name)  \n",
    "\n",
    "    print(f\"✅ Appended {new_df_with_id.count()} rows to {full_table_name} with IDs like '1', '2', '3', ...\")\n",
    "\n",
    "\n",
    "def start_run_cycle(\n",
    "    packagename: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inserts a new row into the run cycle table to mark the start of a cycle.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name: name of the target Delta table\n",
    "    - description: description of this run cycle (string)\n",
    "    - packageid: package identifier (string)\n",
    "    - packagename: package name (string)\n",
    "    - database: optional database name (default: 'default')\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    df_existing = spark.sql(\"SELECT max(cast(runcycleid as int)) as runcycleid FROM dimruncycle\")\n",
    "    runcycleid = df_existing.first().runcycleid + 1\n",
    "    full_table = f\"default.dimruncycle\"\n",
    "\n",
    "    description = \"package: \" + packagename + \" started\"\n",
    "\n",
    "    # Generate a UUID object\n",
    "    uuid_obj = uuid.uuid4()\n",
    "\n",
    "    # Convert the UUID object to a string and make it uppercase\n",
    "    packageid = str(uuid_obj).upper()\n",
    "\n",
    "    # Build single-row DataFrame\n",
    "    data = spark.createDataFrame(\n",
    "        [\n",
    "            (\n",
    "                runcycleid,\n",
    "                None,  # runcyclestartat (will be filled below)\n",
    "                description,\n",
    "                packageid,\n",
    "                packagename,\n",
    "                None,  # runcycleendat\n",
    "                \"NULL\"  # success\n",
    "            )\n",
    "        ],\n",
    "        schema = StructType([\n",
    "                StructField(\"runcycleid\", IntegerType(), False),         # int\n",
    "                StructField(\"runcyclestartat\", TimestampType(), True),   # timestamp\n",
    "                StructField(\"description\", StringType(), True),          # string\n",
    "                StructField(\"packageid\", StringType(), True),            # string\n",
    "                StructField(\"packagename\", StringType(), True),          # string\n",
    "                StructField(\"runcycleendat\", StringType(), True),        # string (you may want to make this a TimestampType too)\n",
    "                StructField(\"success\", StringType(), True),              # string\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Set current timestamp as runcyclestartat\n",
    "    df_with_timestamp = data.withColumn(\"runcyclestartat\", current_timestamp())\n",
    "\n",
    "    # Append to table\n",
    "    df_with_timestamp.write.mode(\"append\").saveAsTable(full_table)\n",
    "\n",
    "    print(f\"✅ Run cycle '{runcycleid}' inserted into {full_table}.\")\n",
    "    return runcycleid\n",
    "\n",
    "def end_run_cycle(\n",
    "    runcycleid: str,\n",
    "    success: str,\n",
    "    packagename: str,\n",
    "    error: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the run cycle row to mark the end of the run.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name: name of the target Delta table\n",
    "    - runcycleid: ID of the run cycle to update\n",
    "    - success: True or False indicating run success\n",
    "    - database: optional database name (default: 'default')\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    full_table = f\"default.dimruncycle\"\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, full_table)\n",
    "\n",
    "    if success == 't':\n",
    "        description = \"package: \" + packagename + \" complete\"\n",
    "    else:        \n",
    "        description = \"package: \" + packagename + \" error \" + error\n",
    "\n",
    "    \n",
    "\n",
    "    # Perform update\n",
    "    delta_table.update(\n",
    "        condition=f\"runcycleid = '{runcycleid}'\",\n",
    "        set={\n",
    "            \"description\": lit(str(description)),\n",
    "            \"runcycleendat\": current_timestamp().cast(\"string\"),\n",
    "            \"success\": lit(str(success).lower()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Run cycle '{runcycleid}' marked as ended with success={success}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4338e633-3b5d-4c34-bd31-d7757cbb4c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# src/utils/job_tracker.py\n",
    "\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "DEFAULT_START_DATE = datetime(2025, 7, 11).date()\n",
    "\n",
    "def _ensure_job_tracker_table_exists(spark: SparkSession, job_tracker_table_path: str):\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hive_metastore.default.job_tracker (\n",
    "            job_name STRING NOT NULL,\n",
    "            run_id STRING NOT NULL,\n",
    "            start_time TIMESTAMP NOT NULL,\n",
    "            end_time TIMESTAMP,\n",
    "            status STRING NOT NULL,\n",
    "            message STRING\n",
    "        ) USING DELTA\n",
    "        LOCATION '{job_tracker_table_path}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.sql(create_table_sql)\n",
    "        print(f\"Ensured job tracker table exists at: {job_tracker_table_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error ensuring job tracker table exists: {e}\")\n",
    "        raise # Re-raise to prevent job from proceeding without tracker\n",
    "\n",
    "\n",
    "def get_last_successful_run_time(spark: SparkSession, job_tracker_table_path: str, job_name: str) -> datetime | None:\n",
    "    try:\n",
    "        _ensure_job_tracker_table_exists(spark, job_tracker_table_path)\n",
    "        tracker_df = spark.read.format(\"delta\").load(job_tracker_table_path)\n",
    "\n",
    "        last_run_df = tracker_df.filter(\n",
    "            (col(\"job_name\") == job_name) & (col(\"status\") == \"SUCCEEDED\")\n",
    "        ).orderBy(col(\"start_time\").desc())\n",
    "\n",
    "        if last_run_df.count() > 0:\n",
    "            last_successful_time = last_run_df.first()[\"start_time\"]\n",
    "            print(f\"Found last successful run for '{job_name}' at: {last_successful_time}\")\n",
    "            return last_successful_time\n",
    "        else:\n",
    "            print(f\"No previous successful run found for '{job_name}'.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading job tracker for last successful run for '{job_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def record_job_status(spark: SparkSession, job_tracker_table_path: str, job_name: str, run_id: str, status: str,\n",
    "                      start_time: datetime, end_time: datetime = None,\n",
    "                      message: str = None):\n",
    "    _ensure_job_tracker_table_exists(spark, job_tracker_table_path) # Ensure table before writing\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"job_name\", StringType(), False),\n",
    "        StructField(\"run_id\", StringType(), False),\n",
    "        StructField(\"start_time\", TimestampType(), False),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"message\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [(job_name, run_id, start_time, end_time, status, message)]\n",
    "\n",
    "    new_status_df = spark.createDataFrame(data, schema=schema)\n",
    "    new_status_df.createOrReplaceTempView(\"new_status_df_temp_view\") # Create a temp view for MERGE\n",
    "\n",
    "    try:\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO delta.`{job_tracker_table_path}` AS target\n",
    "            USING new_status_df_temp_view AS source\n",
    "            ON target.job_name = source.job_name AND target.run_id = source.run_id\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET\n",
    "                    end_time = source.end_time,\n",
    "                    status = source.status,\n",
    "                    message = source.message\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (job_name, run_id, start_time, end_time, status, message)\n",
    "                VALUES (source.job_name, source.run_id, source.start_time, source.end_time, source.status, source.message)\n",
    "        \"\"\"\n",
    "        spark.sql(merge_sql)\n",
    "        print(f\"Job status for '{job_name}' (run_id: {run_id}) recorded as: {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to record job status for '{job_name}' (run_id: {run_id}): {e}\")\n",
    "        raise # Re-raise to ensure the job failure is propagated\n",
    "\n",
    "def get_current_run_id(spark: SparkSession) -> str:\n",
    "    \"\"\"\n",
    "    Get the Databricks run ID from Spark conf, otherwise generates a UUID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.conf.get(\"spark.databricks.driver.runId\")\n",
    "    except Exception:\n",
    "        run_id = str(uuid.uuid4())\n",
    "        print(f\"Warning: spark.databricks.driver.runId not found. Using generated UUID: {run_id}\")\n",
    "        return run_id\n",
    "\n",
    "def generate_date_range_json(last_successful_run_date: datetime | None, current_job_date: datetime) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a JSON array of dates (YYYY-MM-DD) between the last successful run date and the current job date.\n",
    "    \"\"\"\n",
    "    date_list = []\n",
    "    \n",
    "    # Ensure it's date only, not time\n",
    "    end_date = current_job_date.date()\n",
    "\n",
    "    if last_successful_run_date:\n",
    "        # start from a day before the last successful run date\n",
    "        start_date = last_successful_run_date.date()\n",
    "        # Ensure start_date is not after end_date\n",
    "        if start_date > end_date:\n",
    "            start_date = end_date\n",
    "    else:\n",
    "        # If no last successful run, start from DEFAULT_START_DATE.\n",
    "        start_date = DEFAULT_START_DATE\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19887ce4-ffc9-4da8-9d9d-a581a857a2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, DoubleType,\n",
    "    BooleanType, TimestampType, DateType, ArrayType, MapType\n",
    ")\n",
    "from delta.tables import DeltaTable # For merge operations\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from datetime import datetime\n",
    "\n",
    "dbutils.widgets.text(\"bucket\", \"DataStagingBucket\", \"S3 Bucket\")\n",
    "dbutils.widgets.text(\"env\", \"prod\", \"Environment\")\n",
    "\n",
    "# -------------------- Configuration ----------------------\n",
    "JOB_NAME = \"Daily_DOCREVIEWER_DB_Loading\"\n",
    "JOB_TRACKER_TABLE_PATH = \"/mnt/delta/tables/job_tracker\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(f\"{JOB_NAME}_APP\").getOrCreate()\n",
    "\n",
    "# Get the current Databricks run ID (or a generated UUID)\n",
    "current_run_id = get_current_run_id(spark)\n",
    "print(f\"Starting job: {JOB_NAME} with run_id: {current_run_id}\")\n",
    "job_start_timestamp = datetime.now()\n",
    "status_message = None\n",
    "\n",
    "# --- S3 Configuration ---\n",
    "# s3_bucket = spark.conf.get(\"spark.s3_bucket\")\n",
    "s3_bucket = dbutils.widgets.get(\"bucket\")\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "S3_BASE_PATH = f\"s3a://{s3_bucket}/daily_exports/{env}/docreviewer/\"\n",
    "# now = datetime.now()\n",
    "# formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "# S3_BASE_PATH = f\"s3a://{s3_bucket}/daily_exports/dev/docreviewer/{formatted_date}/\"\n",
    "# S3_BASE_PATH = f\"s3a://{s3_bucket}/daily_exports/dev/docreviewer/2025-06-03/\"\n",
    "\n",
    "# Base path for your Delta tables\n",
    "DELTA_BASE_PATH = \"dbfs:/user/hive/warehouse/docreviewer.db\"\n",
    "\n",
    "# Array of S3 JSON file paths.\n",
    "json_files_to_process = [\n",
    "    # \"Annotations.json\",\n",
    "    # \"AnnotationSections.json\",\n",
    "    # \"DeduplicationJob.json\",\n",
    "    # \"DocumentAttributes.json\",\n",
    "    \"DocumentDeleted.json\",\n",
    "    # \"DocumentDeletedPages.json\",\n",
    "    # \"DocumentExtractionJob.json\",\n",
    "    \"DocumentHashCodes.json\",\n",
    "    \"DocumentMaster.json\",\n",
    "    # \"DocumentPageflagHistory.json\",\n",
    "    # \"DocumentPageflags.json\",\n",
    "    # \"DocumentPathMapper.json\",\n",
    "    \"Documents.json\",\n",
    "    # \"DocumentStatus.json\",\n",
    "    # \"FileConversionJob.json\",\n",
    "    # \"Keywords.json\",\n",
    "    # \"OperatingTeamS3ServiceAccounts.json\",\n",
    "    # \"PageCalculatorJob.json\",\n",
    "    # \"PageFlags.json\",\n",
    "    # \"PDFStitchJob.json\",\n",
    "    # \"PDFStitchJobAttributes.json\",\n",
    "    # \"PDFStitchPackage.json\",\n",
    "    # \"ProgramAreaDivisions.json\",\n",
    "    # \"ProgramAreas.json\",\n",
    "    # \"RedactionLayers.json\",\n",
    "    # \"RedlineContents.json\",\n",
    "    # \"Sections.json\",\n",
    "    # # New tables\n",
    "    # \"CompressionJob.json\",\n",
    "    # \"DocumentOCRJob.json\",\n",
    "    # \"DocumentProcesses.json\",\n",
    "    # \"OCRActiveMQJob.json\",\n",
    "    # \"OIRedactionCodes.json\"\n",
    "]\n",
    "\n",
    "# --- table_mapping ---\n",
    "table_mappings = {\n",
    "    \"annotations\": {\n",
    "        \"target_table_name\": \"annotations\",\n",
    "        \"primary_keys\": [\"annotationid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"annotationid\": (\"annotationid\", IntegerType()),\n",
    "            \"annotationname\": (\"annotationname\", StringType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"documentversion\": (\"documentversion\", IntegerType()),\n",
    "            \"annotation\": (\"annotation\", StringType()),\n",
    "            \"pagenumber\": (\"pagenumber\", IntegerType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"redactionlayerid\": (\"redactionlayerid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"annotationsections\": {\n",
    "        \"target_table_name\": \"annotationsections\",\n",
    "        \"primary_keys\": [\"id\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"id\": (\"id\", IntegerType()),\n",
    "            \"annotationname\": (\"annotationname\", StringType()),\n",
    "            \"foiministryrequestid\": (\"foiministryrequestid\", IntegerType()),\n",
    "            \"section\": (\"section\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"redactionlayerid\": (\"redactionlayerid\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"deduplicationjob\": {\n",
    "        \"target_table_name\": \"deduplicationjob\",\n",
    "        \"primary_keys\": [\"deduplicationjobid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"deduplicationjobid\": (\"deduplicationjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"batch\": (\"batch\", StringType()),\n",
    "            \"trigger\": (\"trigger\", StringType()),\n",
    "            \"type\": (\"type\", StringType()),\n",
    "            \"filename\": (\"filename\", StringType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentattributes\": {\n",
    "        \"target_table_name\": \"documentattributes\",\n",
    "        \"primary_keys\": [\"attributeid\"],\n",
    "        \"transform_map\": {\n",
    "            \"attributeid\": (\"attributeid\", IntegerType()),\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"attributes\": (\"attributes\", StringType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentdeleted\": {\n",
    "        \"target_table_name\": \"documentdeleted\",\n",
    "        \"primary_keys\": [\"documentdeletedid\"],\n",
    "        \"transform_map\": {\n",
    "            \"documentdeletedid\": (\"documentdeletedid\", IntegerType()),\n",
    "            \"filepath\": (\"filepath\", StringType()),\n",
    "            \"deleted\": (\"deleted\", StringType()), # Consider BooleanType\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"removedfromsolr\": (\"removedfromsolr\", StringType()), # Consider BooleanType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentdeletedpages\": {\n",
    "        \"target_table_name\": \"documentdeletedpages\",\n",
    "        \"primary_keys\": [\"id\"],\n",
    "        \"transform_map\": {\n",
    "            \"id\": (\"id\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"redactionlayerid\": (\"redactionlayerid\", IntegerType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"pagemetadata\": (\"pagemetadata\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentextractionjob\": {\n",
    "        \"target_table_name\": \"documentextractionjob\",\n",
    "        \"primary_keys\": [\"extractionjobid\"],\n",
    "        \"transform_map\": {\n",
    "            \"extractionjobid\": (\"extractionjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documenthashcodes\": {\n",
    "        \"target_table_name\": \"documenthashcodes\",\n",
    "        \"primary_keys\": [\"documentid\"],\n",
    "        \"transform_map\": {\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"rank1hash\": (\"rank1hash\", StringType()),\n",
    "            \"rank2hash\": (\"rank2hash\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentmaster\": {\n",
    "        \"target_table_name\": \"documentmaster\",\n",
    "        \"primary_keys\": [\"documentmasterid\"],\n",
    "        \"transform_map\": {\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"filepath\": (\"filepath\", StringType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"recordid\": (\"recordid\", StringType()),\n",
    "            \"processingparentid\": (\"processingparentid\", StringType()),\n",
    "            \"parentid\": (\"parentid\", StringType()),\n",
    "            \"isredactionready\": (\"isredactionready\", StringType()), # Consider BooleanType\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentpageflaghistory\": {\n",
    "        \"target_table_name\": \"documentpageflaghistory\",\n",
    "        \"primary_keys\": [\"id\"],\n",
    "        \"transform_map\": {\n",
    "            \"id\": (\"id\", IntegerType()),\n",
    "            \"documentpageflagid\": (\"documentpageflagid\", IntegerType()),\n",
    "            \"foiministryrequestid\": (\"foiministryrequestid\", IntegerType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"documentversion\": (\"documentversion\", IntegerType()),\n",
    "            \"pageflag\": (\"pageflag\", StringType()),\n",
    "            \"attributes\": (\"attributes\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"redactionlayerid\": (\"redactionlayerid\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentpageflags\": {\n",
    "        \"target_table_name\": \"documentpageflags\",\n",
    "        \"primary_keys\": [\"id\"],\n",
    "        \"transform_map\": {\n",
    "            \"id\": (\"id\", IntegerType()),\n",
    "            \"foiministryrequestid\": (\"foiministryrequestid\", IntegerType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"documentversion\": (\"documentversion\", IntegerType()),\n",
    "            \"pageflag\": (\"pageflag\", StringType()),\n",
    "            \"attributes\": (\"attributes\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"redactionlayerid\": (\"redactionlayerid\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentpathmapper\": {\n",
    "        \"target_table_name\": \"documentpathmapper\",\n",
    "        \"primary_keys\": [\"documentpathid\"],\n",
    "        \"transform_map\": {\n",
    "            \"documentpathid\": (\"documentpathid\", IntegerType()),\n",
    "            \"category\": (\"category\", StringType()),\n",
    "            \"bucket\": (\"bucket\", StringType()),\n",
    "            \"attributes\": (\"attributes\", StringType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documents\": {\n",
    "        \"target_table_name\": \"documents\",\n",
    "        \"primary_keys\": [\"documentid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"filename\": (\"filename\", StringType()),\n",
    "            \"attributes\": (\"attributes\", StringType()),\n",
    "            \"foiministryrequestid\": (\"foiministryrequestid\", IntegerType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"statusid\": (\"statusid\", IntegerType()),\n",
    "            \"pagecount\": (\"pagecount\", IntegerType()),\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"incompatible\": (\"incompatible\", StringType()), # Consider BooleanType\n",
    "            \"originalpagecount\": (\"originalpagecount\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentstatus\": {\n",
    "        \"target_table_name\": \"documentstatus\",\n",
    "        \"primary_keys\": [\"statusid\"],\n",
    "        \"transform_map\": {\n",
    "            \"statusid\": (\"statusid\", IntegerType()),\n",
    "            \"name\": (\"name\", StringType()),\n",
    "            \"description\": (\"description\", StringType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"fileconversionjob\": {\n",
    "        \"target_table_name\": \"fileconversionjob\",\n",
    "        \"primary_keys\": [\"fileconversionjobid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"fileconversionjobid\": (\"fileconversionjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"batch\": (\"batch\", StringType()),\n",
    "            \"trigger\": (\"trigger\", StringType()),\n",
    "            \"filename\": (\"filename\", StringType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"inputdocumentmasterid\": (\"inputdocumentmasterid\", IntegerType()),\n",
    "            \"outputdocumentmasterid\": (\"outputdocumentmasterid\", StringType()), # Consider IntegerType if it's an ID\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"keywords\": {\n",
    "        \"target_table_name\": \"keywords\",\n",
    "        \"primary_keys\": [\"keywordid\"],\n",
    "        \"transform_map\": {\n",
    "            \"keywordid\": (\"keywordid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"keyword\": (\"keyword\", StringType()),\n",
    "            \"category\": (\"category\", StringType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"operatingteams3serviceaccounts\": {\n",
    "        \"target_table_name\": \"operatingteams3serviceaccounts\",\n",
    "        \"primary_keys\": [\"teamid\"],\n",
    "        \"transform_map\": {\n",
    "            \"teamid\": (\"teamid\", IntegerType()),\n",
    "            \"usergroup\": (\"usergroup\", StringType()),\n",
    "            \"accesskey\": (\"accesskey\", IntegerType()),\n",
    "            \"secret\": (\"secret\", IntegerType()),\n",
    "            \"type\": (\"type\", StringType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"pagecalculatorjob\": {\n",
    "        \"target_table_name\": \"pagecalculatorjob\",\n",
    "        \"primary_keys\": [\"pagecalculatorjobid\", \"version\"], # Added primary key based on common patterns\n",
    "        \"transform_map\": {\n",
    "            \"pagecalculatorjobid\": (\"pagecalculatorjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"inputmessage\": (\"inputmessage\", StringType()),\n",
    "            \"pagecount\": (\"pagecount\", StringType()), # Consider IntegerType\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"pageflags\": {\n",
    "        \"target_table_name\": \"pageflags\",\n",
    "        \"primary_keys\": [\"pageflagid\"],\n",
    "        \"transform_map\": {\n",
    "            \"pageflagid\": (\"pageflagid\", IntegerType()),\n",
    "            \"name\": (\"name\", StringType()),\n",
    "            \"description\": (\"description\", StringType()),\n",
    "            \"sortorder\": (\"sortorder\", IntegerType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"pdfstitchjob\": {\n",
    "        \"target_table_name\": \"pdfstitchjob\",\n",
    "        \"primary_keys\": [\"pdfstitchjobid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"pdfstitchjobid\": (\"pdfstitchjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"category\": (\"category\", StringType()),\n",
    "            \"inputfiles\": (\"inputfiles\", StringType()),\n",
    "            \"outputfiles\": (\"outputfiles\", StringType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"pdfstitchjobattributes\": {\n",
    "        \"target_table_name\": \"pdfstitchjobattributes\",\n",
    "        \"primary_keys\": [\"attributesid\"],\n",
    "        \"transform_map\": {\n",
    "            \"attributesid\": (\"attributesid\", IntegerType()),\n",
    "            \"pdfstitchjobid\": (\"pdfstitchjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"attributes\": (\"attributes\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"pdfstitchpackage\": {\n",
    "        \"target_table_name\": \"pdfstitchpackage\",\n",
    "        \"primary_keys\": [\"pdfstitchpackageid\"],\n",
    "        \"transform_map\": {\n",
    "            \"pdfstitchpackageid\": (\"pdfstitchpackageid\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"category\": (\"category\", StringType()),\n",
    "            \"finalpackagepath\": (\"finalpackagepath\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"programareadivisions\": {\n",
    "        \"target_table_name\": \"programareadivisions\",\n",
    "        \"primary_keys\": [\"divisionid\"],\n",
    "        \"transform_map\": {\n",
    "            \"divisionid\": (\"divisionid\", IntegerType()),\n",
    "            \"programareaid\": (\"programareaid\", IntegerType()),\n",
    "            \"name\": (\"name\", StringType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"sortorder\": (\"sortorder\", StringType()), # Consider IntegerType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"programareas\": {\n",
    "        \"target_table_name\": \"programareas\",\n",
    "        \"primary_keys\": [\"programareaid\"],\n",
    "        \"transform_map\": {\n",
    "            \"programareaid\": (\"programareaid\", IntegerType()),\n",
    "            \"name\": (\"name\", StringType()),\n",
    "            \"type\": (\"type\", StringType()),\n",
    "            \"bcgovcode\": (\"bcgovcode\", StringType()),\n",
    "            \"iaocode\": (\"iaocode\", StringType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"redactionlayers\": {\n",
    "        \"target_table_name\": \"redactionlayers\",\n",
    "        \"primary_keys\": [\"redactionlayerid\"],\n",
    "        \"transform_map\": {\n",
    "            \"redactionlayerid\": (\"redactionlayerid\", IntegerType()),\n",
    "            \"name\": (\"name\", StringType()),\n",
    "            \"description\": (\"description\", StringType()),\n",
    "            \"sortorder\": (\"sortorder\", IntegerType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"redlinecontents\": {\n",
    "        \"target_table_name\": \"redlinecontents\",\n",
    "        \"primary_keys\": [\"id\"],\n",
    "        \"transform_map\": {\n",
    "            \"id\": (\"id\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"redlineid\": (\"redlineid\", StringType()), # Consider IntegerType if it's an ID\n",
    "            \"annotationid\": (\"annotationid\", StringType()), # Consider IntegerType if it's an ID\n",
    "            \"pagenumber\": (\"pagenumber\", IntegerType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"type\": (\"type\", StringType()),\n",
    "            \"section\": (\"section\", StringType()),\n",
    "            \"content\": (\"content\", StringType()),\n",
    "            \"category\": (\"category\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"sections\": {\n",
    "        \"target_table_name\": \"sections\",\n",
    "        \"primary_keys\": [\"sectionid\"],\n",
    "        \"transform_map\": {\n",
    "            \"sectionid\": (\"sectionid\", IntegerType()),\n",
    "            \"section\": (\"section\", StringType()),\n",
    "            \"description\": (\"description\", StringType()),\n",
    "            \"sortorder\": (\"sortorder\", IntegerType()),\n",
    "            \"isactive\": (\"isactive\", StringType()), # Consider BooleanType\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()), # Changed to TimestampType\n",
    "            \"updatedby\": (\"updatedby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    # New tables\n",
    "    # \"OCRActiveMQJob.json\"\n",
    "    \"compressionjob\": {\n",
    "        \"target_table_name\": \"compressionjob\",\n",
    "        \"primary_keys\": [\"compressionjobid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"compressionjobid\": (\"compressionjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"batch\": (\"batch\", StringType()),\n",
    "            \"trigger\": (\"trigger\", StringType()),\n",
    "            \"filename\": (\"filename\", StringType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentocrjob\": {\n",
    "        \"target_table_name\": \"documentocrjob\",\n",
    "        \"primary_keys\": [\"ocrjobid\"],\n",
    "        \"transform_map\": {\n",
    "            \"ocrjobid\": (\"ocrjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"documentid\": (\"documentid\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"documentprocesses\": {\n",
    "        \"target_table_name\": \"documentprocesses\",\n",
    "        \"primary_keys\": [\"processid\"],\n",
    "        \"transform_map\": {\n",
    "            \"processid\": (\"processid\", IntegerType()),\n",
    "            \"name\": (\"name\", StringType()),\n",
    "            \"description\": (\"description\", StringType()),\n",
    "            \"isactive\": (\"isactive\", BooleanType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"ocractivemqjob\": {\n",
    "        \"target_table_name\": \"ocractivemqjob\",\n",
    "        \"primary_keys\": [\"ocractivemqjobid\", \"version\"],\n",
    "        \"transform_map\": {\n",
    "            \"ocractivemqjobid\": (\"ocractivemqjobid\", IntegerType()),\n",
    "            \"version\": (\"version\", IntegerType()),\n",
    "            \"ministryrequestid\": (\"ministryrequestid\", IntegerType()),\n",
    "            \"createdat\": (\"createdat\", TimestampType()),\n",
    "            \"batch\": (\"batch\", StringType()),\n",
    "            \"trigger\": (\"trigger\", StringType()),\n",
    "            \"filename\": (\"filename\", StringType()),\n",
    "            \"status\": (\"status\", StringType()),\n",
    "            \"message\": (\"message\", StringType()),\n",
    "            \"documentmasterid\": (\"documentmasterid\", IntegerType()),\n",
    "            \"load_timestamp\": (None, TimestampType(), lambda: current_timestamp())\n",
    "        }\n",
    "    },\n",
    "    \"oiredactioncodes\": {\n",
    "        \"target_table_name\": \"oiredactioncodes\",\n",
    "        \"primary_keys\": [\"redactioncodeid\"],\n",
    "        \"transform_map\": {\n",
    "            \"redactioncodeid\": (\"redactioncodeid\", IntegerType()),\n",
    "            \"redactioncode\": (\"redactioncode\", StringType()),\n",
    "            \"description\": (\"description\", StringType()),\n",
    "            \"sortorder\": (\"sortorder\", IntegerType()),\n",
    "            \"isactive\": (\"isactive\", BooleanType()),\n",
    "            \"created_at\": (\"created_at\", TimestampType()),\n",
    "            \"createdby\": (\"createdby\", StringType()),\n",
    "            \"updated_at\": (\"updated_at\", TimestampType()),\n",
    "            \"updatedby\": (\"updatedby\", StringType())\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2ea1458a-9f0e-4ac5-a5d6-965cf746bf35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "from pyspark.sql.functions import col, lit, to_json\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType\n",
    "\n",
    "def get_table_config_from_path(file_path, mappings):\n",
    "    for key, config in mappings.items():\n",
    "        if (f\"{key}.json\") in file_path.lower():\n",
    "            return config\n",
    "    raise ValueError(f\"No table mapping found for file path: {file_path}\")\n",
    "\n",
    "def get_dynamic_select_expressions(source_df, target_schema_actual: StructType, transform_map):\n",
    "    select_exprs = []\n",
    "    source_schema_dict = {f.name.lower(): f.dataType for f in source_df.schema.fields}\n",
    "\n",
    "    for target_field in target_schema_actual.fields:\n",
    "        target_col_name = target_field.name\n",
    "        target_type = target_field.dataType\n",
    "        transform_rule = transform_map.get(target_col_name)\n",
    "\n",
    "        if transform_rule:\n",
    "            src_col_name_or_none, rule_target_type, *custom_transform_func = transform_rule\n",
    "            \n",
    "            if custom_transform_func:\n",
    "                select_exprs.append(custom_transform_func[0]().alias(target_col_name))\n",
    "            elif src_col_name_or_none:\n",
    "                # Resolve the source column name\n",
    "                clean_src_name = src_col_name_or_none.lower()\n",
    "                \n",
    "                if clean_src_name in source_schema_dict or \".\" in src_col_name_or_none:\n",
    "                    # Get the source data type\n",
    "                    source_type = source_schema_dict.get(clean_src_name)\n",
    "                    \n",
    "                    # Check if source is complex (Array/Map/Struct) and target is String\n",
    "                    is_complex_source = isinstance(source_type, (ArrayType, MapType, StructType))\n",
    "                    is_target_string = isinstance(target_type, StringType)\n",
    "\n",
    "                    if is_complex_source and is_target_string:\n",
    "                        # Use to_json to maintain valid JSON formatting\n",
    "                        select_exprs.append(to_json(col(src_col_name_or_none)).alias(target_col_name))\n",
    "                    else:\n",
    "                        select_exprs.append(col(src_col_name_or_none).cast(target_type).alias(target_col_name))\n",
    "                else:\n",
    "                    print(f\"Warning: Source column '{src_col_name_or_none}' not found. Adding as NULL.\")\n",
    "                    select_exprs.append(lit(None).cast(target_type).alias(target_col_name))\n",
    "            else:\n",
    "                select_exprs.append(lit(None).cast(target_type).alias(target_col_name))\n",
    "        else:\n",
    "            # Fallback for columns not in transform_map\n",
    "            if target_col_name.lower() in source_schema_dict:\n",
    "                source_type = source_schema_dict.get(target_col_name.lower())\n",
    "                if isinstance(source_type, (ArrayType, MapType, StructType)) and isinstance(target_type, StringType):\n",
    "                    select_exprs.append(to_json(col(target_col_name)).alias(target_col_name))\n",
    "                else:\n",
    "                    select_exprs.append(col(target_col_name).cast(target_type).alias(target_col_name))\n",
    "            else:\n",
    "                select_exprs.append(lit(None).cast(target_type).alias(target_col_name))\n",
    "            \n",
    "    return select_exprs\n",
    "\n",
    "def create_or_get_delta_table(spark_session, base_path, table_name):\n",
    "    \"\"\"\n",
    "    Retrieves the DeltaTable object and its schema if it exists.\n",
    "    If the table does not exist, it returns (None, None).\n",
    "    \"\"\"\n",
    "    full_table_path = f\"{base_path}/{table_name}\"\n",
    "    \n",
    "    if not DeltaTable.isDeltaTable(spark_session, full_table_path):\n",
    "        print(f\"  Delta table '{table_name}' does NOT exist at {full_table_path}. Skipping this file.\")\n",
    "        return None, None # Signal that the table was not found\n",
    "    else:\n",
    "        print(f\"  Delta table '{table_name}' already exists at {full_table_path}. Retrieving schema.\")\n",
    "        actual_table_schema = spark_session.read.format(\"delta\").load(full_table_path).schema\n",
    "        print(f\"  Retrieved actual schema for {table_name}:\")\n",
    "        spark_session.createDataFrame([], actual_table_schema).printSchema()\n",
    "        delta_table_ref = DeltaTable.forPath(spark_session, full_table_path)\n",
    "        return delta_table_ref, actual_table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61414053-da33-40ed-93a0-26fde0038dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    last_successful_run_time = get_last_successful_run_time(spark, JOB_TRACKER_TABLE_PATH, JOB_NAME)\n",
    "    # date_range = generate_date_range_json(last_successful_run_time, job_start_timestamp)\n",
    "    date_range = generate_date_range_json(datetime.strptime('2025-07-11', '%Y-%m-%d'), datetime.strptime('2026-02-25', '%Y-%m-%d'))\n",
    "    # print(f\"Type of date_range ({date_range}): {type(date_range)}\")\n",
    "\n",
    "    # Main Processing Loop\n",
    "    print(f\"Starting JSON to Delta merge process... Date range: {date_range}\")\n",
    "\n",
    "    for date_str in date_range:\n",
    "        print(f\"Processing data for date: {date_str}\")\n",
    "\n",
    "        for json_file_relative_path in json_files_to_process:\n",
    "            full_json_path = f\"{S3_BASE_PATH}{date_str}/{json_file_relative_path}\"\n",
    "            print(f\"\\n--- Processing file: {full_json_path} ---\")\n",
    "\n",
    "            try:\n",
    "                # 1. get target table configuration\n",
    "                table_config = get_table_config_from_path(json_file_relative_path, table_mappings)\n",
    "                target_table_name = table_config[\"target_table_name\"]\n",
    "                primary_keys = table_config[\"primary_keys\"]\n",
    "                transform_map = table_config[\"transform_map\"]\n",
    "                \n",
    "                full_delta_table_path = f\"{DELTA_BASE_PATH}/{target_table_name}\"\n",
    "\n",
    "                print(f\"  Target table: {target_table_name} at {full_delta_table_path}\")\n",
    "                print(f\"  Primary Keys: {primary_keys}\")\n",
    "\n",
    "                # 2. Get the Delta table reference and schema\n",
    "                delta_table, actual_target_schema = create_or_get_delta_table(\n",
    "                    spark, DELTA_BASE_PATH, target_table_name # No initial_schema_for_creation passed here\n",
    "                )\n",
    "                \n",
    "                # --- Skip if table does not exist ---\n",
    "                if delta_table is None:\n",
    "                    continue # Skip to the next file in the loop\n",
    "\n",
    "                # 3. Read the JSON file\n",
    "                try:\n",
    "                    source_df = spark.read.json(full_json_path)\n",
    "                    print(\"  Source JSON schema (inferred):\")\n",
    "                    source_df.printSchema()\n",
    "                except Exception as e:\n",
    "                    # Skip to next file/table if JSON file not exist\n",
    "                    if \"Path does not exist\" in str(e) or \"No such file or directory\" in str(e):\n",
    "                        print(f\"S3 JSON file '{full_json_path}' not found. Skipping this file. {e}\")\n",
    "                        continue # Skip to the next file in the loop\n",
    "                    else:\n",
    "                        print(f\"Other S3 JSON file '{full_json_path}' error. Escalate issue. {e}\")\n",
    "                        raise e\n",
    "\n",
    "                # 4. Transform source data to match target schema\n",
    "                select_expressions = get_dynamic_select_expressions(source_df, actual_target_schema, transform_map)\n",
    "                \n",
    "                transformed_df = source_df.select(*select_expressions)\n",
    "                print(\"  Transformed DataFrame schema (before merge):\")\n",
    "                transformed_df.printSchema()\n",
    "                \n",
    "                # 5. UPSERT/MERGE\n",
    "                merge_condition = \" AND \".join([f\"target.{pk} = source.{pk}\" for pk in primary_keys])\n",
    "\n",
    "                print(f\"  Merge condition: {merge_condition}\")\n",
    "                \n",
    "                (delta_table.alias(\"target\")\n",
    "                    .merge(\n",
    "                        transformed_df.alias(\"source\"),\n",
    "                        merge_condition\n",
    "                    )\n",
    "                    .whenMatchedUpdateAll()\n",
    "                    .whenNotMatchedInsertAll()\n",
    "                    .execute()\n",
    "                )\n",
    "                print(f\"  Successfully merged data from {json_file_relative_path} into {target_table_name}.\")\n",
    "\n",
    "            except ValueError as ve:\n",
    "                print(f\"Error processing {json_file_relative_path}: {ve}\")\n",
    "                # raise ve\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred for {json_file_relative_path}: {e}\")\n",
    "                raise e\n",
    "            \n",
    "        print(f\"Processing date: {date_str} - completed\")\n",
    "\n",
    "    print(\"\\nJSON to Delta merge process completed.\")\n",
    "    job_status = \"SUCCEEDED\"\n",
    "    status_message = f\"Job completed successfully. Dates processed: {date_range}\" # Optionally add to message\n",
    "\n",
    "except Exception as e:\n",
    "    job_status = \"FAILED\"\n",
    "    status_message = f\"Job failed: {e}\"\n",
    "    print(f\"ERROR: {status_message}\")\n",
    "    # sys.exit(1) # Exit with a non-zero code to indicate failure in Databricks Jobs\n",
    "\n",
    "finally:\n",
    "    # 3. After the job, save job status (add end time and update the status)\n",
    "    job_end_timestamp = datetime.now()\n",
    "    \n",
    "    record_job_status(\n",
    "        spark=spark,\n",
    "        job_tracker_table_path=JOB_TRACKER_TABLE_PATH,\n",
    "        job_name=JOB_NAME,\n",
    "        run_id=current_run_id,\n",
    "        status=job_status,\n",
    "        start_time=job_start_timestamp,\n",
    "        end_time=job_end_timestamp,\n",
    "        message=status_message # Include the message with date range if desired\n",
    "    )\n",
    "    \n",
    "    # spark.stop()\n",
    "    print(f\"Job '{JOB_NAME}' (run_id: {current_run_id}) finished with status: {job_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02e7e36-7a50-4741-b24c-9df16c6c4ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "\n",
    "# select * from docreviewer.documentpageflags\n",
    "# order by id desc\n",
    "# limit 100;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DB Loading Job_Docreviewer_Missing_ETL",
   "widgets": {
    "bucket": {
     "currentValue": "DataStagingBucket",
     "nuid": "d56e46df-7eb8-4208-aa41-d0777a3bad9e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "DataStagingBucket",
      "label": "S3 Bucket",
      "name": "bucket",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "DataStagingBucket",
      "label": "S3 Bucket",
      "name": "bucket",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "env": {
     "currentValue": "prod",
     "nuid": "a87c91f7-dc72-4dcf-b590-0c2f5960181b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prod",
      "label": "Environment",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "prod",
      "label": "Environment",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
