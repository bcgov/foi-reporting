{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83e46682-573d-4da4-83dc-28202ddb7972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run /Workspace/Shared/etl_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5066fd-ef21-4d97-8d9d-e45749e4fa57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n",
    "%pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.insert(0, '/Workspace/Shared')\n",
    "import etl_helpers \n",
    "from pyspark.sql.functions import collect_list, concat_ws\n",
    "\n",
    "runcycleid = etl_helpers.start_run_cycle(\"factRequestDocumentsDetails\")\n",
    "\n",
    "os.makedirs(\"/dbfs/foi/dataload\", exist_ok=True)  # make sure directory exists\n",
    "\n",
    "try:\n",
    "    # Performance Optimizations\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"auto\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "    # Get last successful run time\n",
    "    df_lastrun = spark.sql(f\"\"\"\n",
    "        SELECT runcyclestartat as createddate \n",
    "        FROM dimruncycle \n",
    "        WHERE packagename = 'factRequestDocumentsDetails' AND success = 't' \n",
    "        ORDER BY runcycleid DESC LIMIT 1\n",
    "    \"\"\")\n",
    "    \n",
    "    if df_lastrun.count() > 0:\n",
    "        maxcreatedate_str = df_lastrun.first().createddate.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        maxcreatedate_str = \"2025-07-11 00:00:00\"\n",
    "    \n",
    "    # maxcreatedate_str = \"2026-02-01 00:00:00\"\n",
    "    print(f\"Incremental Load Start Date: {maxcreatedate_str}\")\n",
    "\n",
    "    # Identify changed requests and Checkpoint\n",
    "    # localCheckpoint() breaks the lineage, preventing the Shuffle Metadata error\n",
    "    df_existing = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT foiministryrequestid, foirequest_id\n",
    "        FROM foi_mod.foiministryrequests\n",
    "        WHERE created_at > '{maxcreatedate_str}' OR try_cast(updated_at AS DATE) > '{maxcreatedate_str}'\n",
    "    \"\"\")\n",
    "    \n",
    "    df_existing = df_existing.localCheckpoint()\n",
    "    df_existing.createOrReplaceTempView(\"temp_requests\")\n",
    "    \n",
    "    change_count = df_existing.count()\n",
    "    print(f\"Records to process: {change_count}\")\n",
    "\n",
    "    if change_count == 0:\n",
    "        raise Exception(\"no changes for today\")\n",
    "\n",
    "    # Set existing records to inactive (Soft Delete)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO default.factRequestDocumentsDetails dd\n",
    "        USING (SELECT foirequest_id FROM temp_requests) AS temp\n",
    "        ON temp.foirequest_id = dd.foirequestid\n",
    "        WHEN MATCHED AND dd.sourceoftruth = 'FOIMOD' THEN\n",
    "            UPDATE SET dd.activeflag = 'N'\n",
    "    \"\"\")\n",
    "\n",
    "    # Optimized CTE Query\n",
    "    query = f\"\"\"\n",
    "    WITH LatestRequests AS (\n",
    "        SELECT foiministryrequestid, requeststatuslabel, created_at, updated_at, foirequest_id\n",
    "        FROM (\n",
    "            SELECT fmr.*, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY fmr.foiministryrequestid ORDER BY version DESC) AS rn\n",
    "            FROM foi_mod.FOIMinistryRequests fmr\n",
    "            INNER JOIN temp_requests tr ON fmr.foiministryrequestid = tr.foiministryrequestid\n",
    "        ) WHERE rn = 1\n",
    "    ),\n",
    "    PageFlags AS (\n",
    "        SELECT\n",
    "            dpf.foiministryrequestid,\n",
    "            SUM(SIZE(FROM_JSON(pageflag, 'ARRAY<STRUCT<flagid: INT, page: INT, programareaid: ARRAY<INT>, other: ARRAY<STRING>>>'))) AS reviewed_count\n",
    "        FROM docreviewer.documentpageflags dpf\n",
    "        INNER JOIN temp_requests tr ON tr.foiministryrequestid = dpf.foiministryrequestid\n",
    "        INNER JOIN docreviewer.documents d ON dpf.documentid = d.documentid\n",
    "        INNER JOIN docreviewer.documentmaster dm ON dm.documentmasterid = d.documentmasterid\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM docreviewer.documentdeleted dd \n",
    "            WHERE dm.ministryrequestid = dd.ministryrequestid AND dm.filepath >= dd.filepath AND dm.filepath < CONCAT(dd.filepath, 'z')\n",
    "        )\n",
    "        GROUP BY dpf.foiministryrequestid\n",
    "    ),\n",
    "    Fees AS (\n",
    "        SELECT \n",
    "            ministryrequestid,\n",
    "            est_elec,\n",
    "            est_hard\n",
    "        FROM (\n",
    "            SELECT \n",
    "                ministryrequestid,\n",
    "                get_json_object(feedata, '$.estimatedelectronicpages') AS est_elec,\n",
    "                get_json_object(feedata, '$.estimatedhardcopypages') AS est_hard,\n",
    "                ROW_NUMBER() OVER (PARTITION BY ministryrequestid ORDER BY updated_at DESC, version DESC) AS rn\n",
    "            FROM foi_mod.foirequestcfrfees\n",
    "        ) WHERE rn = 1\n",
    "    ),\n",
    "    UniqueDocuments AS (\n",
    "        SELECT dhc.rank1hash, MIN(dhc.documentid) AS canonical_doc_id\n",
    "        FROM docreviewer.documenthashcodes dhc\n",
    "        GROUP BY dhc.rank1hash\n",
    "    ),\n",
    "    PageStats AS (\n",
    "        SELECT \n",
    "            d.foiministryrequestid,\n",
    "            SUM(d.pagecount) AS total_pagecount,\n",
    "            SUM(CASE WHEN ud.canonical_doc_id IS NOT NULL THEN d.pagecount ELSE 0 END) AS dedupe_pagecount\n",
    "        FROM docreviewer.documents d\n",
    "        INNER JOIN temp_requests tr ON d.foiministryrequestid = tr.foiministryrequestid\n",
    "        LEFT JOIN UniqueDocuments ud ON d.documentid = ud.canonical_doc_id\n",
    "        GROUP BY d.foiministryrequestid\n",
    "    )\n",
    "    SELECT\n",
    "        lr.foiministryrequestid,\n",
    "        COALESCE(pf.reviewed_count, 0) AS reviewed_count,\n",
    "        CASE WHEN lr.requeststatuslabel = 'closed' THEN COALESCE(pf.reviewed_count, 0) ELSE 0 END AS pagesreleased,\n",
    "        lr.created_at,\n",
    "        lr.updated_at,\n",
    "        lr.foirequest_id,\n",
    "        f.est_elec,\n",
    "        f.est_hard,\n",
    "        ps.total_pagecount,\n",
    "        ps.dedupe_pagecount\n",
    "    FROM LatestRequests lr\n",
    "    LEFT JOIN PageFlags pf ON lr.foiministryrequestid = pf.foiministryrequestid\n",
    "    LEFT JOIN Fees f ON lr.foiministryrequestid = f.ministryrequestid\n",
    "    LEFT JOIN PageStats ps ON lr.foiministryrequestid = ps.foiministryrequestid\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.sql(query)\n",
    "\n",
    "    # Map to Target Schema\n",
    "    df_mapped = df.selectExpr(\n",
    "        \"foirequest_id AS foirequestid\",\n",
    "        f\"{runcycleid} as runcycleid\",\n",
    "        # \"'0' as runcycleid\",\n",
    "        \"reviewed_count AS noofpagesreviewed\",\n",
    "        \"pagesreleased AS noofpagesreleased\",\n",
    "        \"dedupe_pagecount AS noofpagesdeduplicated\",\n",
    "        \"total_pagecount AS noofpagesintherequest\",\n",
    "        \"est_elec AS electronicpageestimate\",\n",
    "        \"est_hard AS physicalpageestimate\",\n",
    "        \"'' AS noofpagesinreviewlog\",\n",
    "        \"'' AS noofpagesinredactionlayer\",\n",
    "        \"'Y' AS activeflag\",\n",
    "        \"'FOIMOD' AS sourceoftruth\"\n",
    "    )\n",
    "\n",
    "    # # --- Debugging ---\n",
    "    # print(\"DEBUG: Showing top 20 results (Write skipped)\")\n",
    "    # display(df_mapped)\n",
    "\n",
    "    # Write Data\n",
    "    df_mapped.write.format(\"delta\").mode(\"append\").insertInto(\"factrequestdocumentsdetails\")\n",
    "    etl_helpers.end_run_cycle(runcycleid, 't', \"factRequestDocumentsDetails\")\n",
    "    print(\"ETL Job Successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"no changes for today\" in error_msg:\n",
    "        print(\"No changes to process.\")\n",
    "        etl_helpers.end_run_cycle(runcycleid, 't', \"factRequestDocumentsDetails\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {error_msg}\")    \n",
    "        etl_helpers.end_run_cycle(runcycleid, 'f', \"factRequestDocumentsDetails\", error_msg)\n",
    "        raise Exception(\"Notebook failed\") from e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "factRequestDocumentsDetailsETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
