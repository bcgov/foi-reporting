{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e46682-573d-4da4-83dc-28202ddb7972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run /Workspace/Shared/etl_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5066fd-ef21-4d97-8d9d-e45749e4fa57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n",
    "%pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.insert(0, '/Workspace/Shared')\n",
    "import etl_helpers \n",
    "from pyspark.sql.functions import collect_list, concat_ws\n",
    "\n",
    "runcycleid = etl_helpers.start_run_cycle(\"factRequestDocumentsDetails\")\n",
    "\n",
    "os.makedirs(\"/dbfs/foi/dataload\", exist_ok=True)  # make sure directory exists\n",
    "\n",
    "try:\n",
    "    # today = str(datetime.date.today())\n",
    "\n",
    "    # last successful run time\n",
    "    df_lastrun = spark.sql(f\"SELECT runcyclestartat as createddate FROM dimruncycle WHERE packagename = 'factRequestDocumentsDetails' AND success = 't' ORDER BY runcycleid DESC LIMIT 1\")\n",
    "    \n",
    "    if df_lastrun.count() > 0:\n",
    "        maxcreatedate_str = df_lastrun.first().createddate.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        # maxcreatedate_str = \"2019-01-01 00:00:00\"\n",
    "        maxcreatedate_str = \"2025-07-11 00:00:00\"\n",
    "    # maxcreatedate_str = \"2025-08-06 00:00:00\"\n",
    "    print(maxcreatedate_str)\n",
    "\n",
    "    # temp table/view for new/update requestids\n",
    "    df_existing = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            foiministryrequestid,\n",
    "            foirequest_id\n",
    "        FROM\n",
    "            foi_mod.foiministryrequests\n",
    "        WHERE\n",
    "            created_at > '{maxcreatedate_str}' OR try_cast(updated_at AS DATE) > '{maxcreatedate_str}'\n",
    "    \"\"\")\n",
    "    df_existing.createOrReplaceTempView(\"temp_requests\")\n",
    "    print(df_existing.count())\n",
    "    df_existing.show()\n",
    "\n",
    "    if df_existing.count() == 0:\n",
    "        raise Exception(\"no changes for today\")\n",
    "\n",
    "    # set existing records to inactive\n",
    "    df_existing = spark.sql(f\"\"\"MERGE INTO default.factRequestDocumentsDetails dd\n",
    "        USING (\n",
    "            SELECT foirequest_id AS foirequestid\n",
    "            FROM temp_requests\n",
    "        ) AS temp\n",
    "        ON temp.foirequestid = dd.foirequestid\n",
    "        WHEN MATCHED and dd.sourceoftruth = 'FOIMOD' THEN\n",
    "\n",
    "        UPDATE \n",
    "        SET dd.activeflag = 'N'\n",
    "    \"\"\");\n",
    "    df_existing.show()\n",
    "\n",
    "\n",
    "    query = f\"\"\"\n",
    "        select\n",
    "            sq1.foiministryrequestid,\n",
    "            count, \n",
    "            case when requeststatuslabel = 'closed' then count else 0 end as pagesreleased,\n",
    "            sq2.created_at,\n",
    "            sq2.updated_at,\n",
    "            sq2.foirequest_id,\n",
    "            estimatedelectronicpages,\n",
    "            estimatedhardcopypages,\n",
    "            pagecount,\n",
    "            dedupepagecount\n",
    "        from \n",
    "        (\n",
    "          (\n",
    "            SELECT\n",
    "                dpf.foiministryrequestid,\n",
    "                SUM(\n",
    "                    SIZE(\n",
    "                        FROM_JSON(\n",
    "                            pageflag,\n",
    "                            'ARRAY<STRUCT<flagid: INT, page: INT, programareaid: ARRAY<INT>, other: ARRAY<STRING>>>'\n",
    "                        )\n",
    "                    )\n",
    "                ) AS count\n",
    "            FROM docreviewer.documentpageflags dpf\n",
    "            JOIN temp_requests ON temp_requests.foiministryrequestid = dpf.foiministryrequestid\n",
    "            JOIN docreviewer.documents d ON dpf.documentid = d.documentid\n",
    "            JOIN docreviewer.documentmaster dm ON dm.documentmasterid = d.documentmasterid\n",
    "            LEFT JOIN docreviewer.documentdeleted dd ON dd.filepath LIKE CONCAT(dm.filepath, '%')\n",
    "            WHERE dd.deleted IS NULL OR dd.deleted IS FALSE\n",
    "            GROUP BY\n",
    "                dpf.foiministryrequestid\n",
    "          ) sq1\n",
    "\n",
    "          join (\n",
    "            WITH ranked AS (\n",
    "              SELECT\n",
    "                *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY foiministryrequestid ORDER BY version DESC) AS rn\n",
    "              FROM foi_mod.FOIMinistryRequests fmr\n",
    "            )\n",
    "            SELECT ranked.foiministryrequestid, ranked.requeststatuslabel, ranked.created_at, ranked.updated_at, ranked.foirequest_id\n",
    "            FROM ranked\n",
    "            join temp_requests on temp_requests.foiministryrequestid = ranked.foiministryrequestid\n",
    "            WHERE rn = 1\n",
    "          ) sq2 on sq1.foiministryrequestid = sq2.foiministryrequestid\n",
    "        )\n",
    "\n",
    "        left join\n",
    "        (\n",
    "          WITH ranked AS (\n",
    "            SELECT\n",
    "              ministryrequestid,\n",
    "              get_json_object(feedata, '$.estimatedelectronicpages') AS estimatedelectronicpages,\n",
    "              get_json_object(feedata, '$.estimatedhardcopypages') AS estimatedhardcopypages,\n",
    "              ROW_NUMBER() OVER (PARTITION BY cfrfeeid ORDER BY version DESC) AS rn\n",
    "            FROM foi_mod.foirequestcfrfees\n",
    "          )\n",
    "          SELECT *\n",
    "          FROM ranked\n",
    "          WHERE rn = 1\n",
    "        ) sq3 on sq3.ministryrequestid = sq1.foiministryrequestid\n",
    "\n",
    "        join (\n",
    "          select sq3.*, sq2.dedupepagecount\n",
    "          from \n",
    "          (\n",
    "            select foiministryrequestid, sum(pagecount) as pagecount from docreviewer.documents d\n",
    "            group by foiministryrequestid\n",
    "          ) sq3\n",
    "          join\n",
    "          (\n",
    "            select sum(pagecount) as dedupepagecount, foiministryrequestid\n",
    "            from docreviewer.documents d1\n",
    "            join \n",
    "            (\n",
    "              select rank1hash, min(d.documentid) as docid  from docreviewer.documenthashcodes dhc\n",
    "              join docreviewer.documents d on d.documentid = dhc.documentid\n",
    "              group by rank1hash\n",
    "            ) sq on sq.docid = d1.documentid\n",
    "            group by foiministryrequestid\n",
    "          ) sq2 on sq2.foiministryrequestid = sq3.foiministryrequestid\n",
    "        ) sq4 on sq4.foiministryrequestid = sq1.foiministryrequestid\n",
    "        \"\"\"\n",
    "\n",
    "    print(query)\n",
    "\n",
    "    df = spark.sql(query)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    # order of columns here is important!\n",
    "    df_mapped = df.selectExpr(\n",
    "        \"foirequest_id AS foirequestid\",\n",
    "        f\"{runcycleid} as runcycleid\",\n",
    "        \"count AS noofpagesreviewed\",\n",
    "        \"pagesreleased AS noofpagesreleased\",\n",
    "        \"dedupepagecount AS noofpagesdeduplicated\",\n",
    "        \"NULL AS noofpagesintherequest\",\n",
    "        \"estimatedelectronicpages AS electronicpageestimate\",\n",
    "        \"estimatedhardcopypages AS physicalpageestimate\",\n",
    "        \"'' AS noofpagesinreviewlog\",\n",
    "        \"'' AS noofpagesinredactionlayer\",\n",
    "        \"'Y' AS activeflag\",\n",
    "        \"'FOIMOD' AS sourceoftruth\",\n",
    "    )\n",
    "    df_mapped.show()\n",
    "    df_mapped.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"false\").insertInto(\"factrequestdocumentsdetails\")  \n",
    "    etl_helpers.end_run_cycle(runcycleid, 't', \"factRequestDocumentsDetails\")\n",
    "except NoCredentialsError:\n",
    "    print(\"Credentials not available\")\n",
    "    etl_helpers.end_run_cycle(runcycleid, 'f', \"factRequestDocumentsDetails\", \"Credentials not available\")\n",
    "    raise Exception(\"notebook failed\") from e\n",
    "except Exception as e:\n",
    "    if (str(e) == \"no changes for today\"):\n",
    "        etl_helpers.end_run_cycle(runcycleid, 't', \"factRequestDocumentsDetails\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {e}\")    \n",
    "        etl_helpers.end_run_cycle(runcycleid, 'f', \"factRequestDocumentsDetails\", f\"An error occurred: {e}\")\n",
    "        raise Exception(\"notebook failed\") from e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "factRequestDocumentsDetailsETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
