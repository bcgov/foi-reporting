{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0ac72bc4-fdd3-403f-b2c6-06c43552eeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import uuid\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "\n",
    "def append_with_incrementing_id(\n",
    "    new_df: DataFrame,\n",
    "    table_name: str,\n",
    "    id_column: str = \"id\",\n",
    "    order_by_column: str = None,\n",
    "    database: str = \"default\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends new_df to a Delta table with string auto-increment IDs like '1', '2', '3', ...\n",
    "\n",
    "    Parameters:\n",
    "    - new_df: The new data to insert.\n",
    "    - table_name: Target Delta table name (must exist).\n",
    "    - id_column: Name of the ID column (default: 'id').\n",
    "    - order_by_column: Optional: Column to use for ordering (for deterministic IDs).\n",
    "    - database: Databricks database containing the table.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    full_table_name = f\"{database}.{table_name}\"\n",
    "\n",
    "    if not spark._jsparkSession.catalog().tableExists(full_table_name):\n",
    "        raise Exception(f\"Table {full_table_name} does not exist. Please create it first.\")\n",
    "\n",
    "    existing_df = spark.table(full_table_name)\n",
    "\n",
    "    if id_column in existing_df.columns:\n",
    "        numeric_part_expr = F.regexp_extract(F.col(id_column), r\"(\\d+)$\", 1).cast(\"long\")\n",
    "        max_id_row = existing_df.select(F.max(numeric_part_expr)).collect()[0][0]\n",
    "        max_id = max_id_row if max_id_row is not None else 0\n",
    "    else:\n",
    "        max_id = 0\n",
    "\n",
    "    if order_by_column and order_by_column in new_df.columns:\n",
    "        windowSpec = Window.orderBy(F.col(order_by_column))\n",
    "    else:\n",
    "        windowSpec = Window.orderBy(F.lit(1))\n",
    "\n",
    "    new_df_with_number = new_df.withColumn(\n",
    "        \"__rownum\", F.row_number().over(windowSpec) + max_id\n",
    "    )\n",
    "\n",
    "    new_df_with_id = new_df_with_number.withColumn(\n",
    "        id_column, F.col(\"__rownum\")\n",
    "    ).drop(\"__rownum\")\n",
    "\n",
    "    cols = new_df_with_id.columns\n",
    "    ordered_cols = [id_column] + [c for c in cols if c != id_column]\n",
    "    new_df_with_id = new_df_with_id.select(ordered_cols)\n",
    "\n",
    "    # new_df_with_id.write.mode(\"append\").saveAsTable(full_table_name)\n",
    "\n",
    "    new_df_with_id.printSchema()\n",
    "    \n",
    "    new_df_with_id.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"false\").insertInto(full_table_name)  \n",
    "\n",
    "    print(f\"✅ Appended {new_df_with_id.count()} rows to {full_table_name} with IDs like '1', '2', '3', ...\")\n",
    "\n",
    "\n",
    "def start_run_cycle(\n",
    "    packagename: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inserts a new row into the run cycle table to mark the start of a cycle.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name: name of the target Delta table\n",
    "    - description: description of this run cycle (string)\n",
    "    - packageid: package identifier (string)\n",
    "    - packagename: package name (string)\n",
    "    - database: optional database name (default: 'default')\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    df_existing = spark.sql(\"SELECT max(cast(runcycleid as int)) as runcycleid FROM dimruncycle\")\n",
    "    runcycleid = df_existing.first().runcycleid + 1\n",
    "    full_table = f\"default.dimruncycle\"\n",
    "\n",
    "    description = \"package: \" + packagename + \" started\"\n",
    "\n",
    "    # Generate a UUID object\n",
    "    uuid_obj = uuid.uuid4()\n",
    "\n",
    "    # Convert the UUID object to a string and make it uppercase\n",
    "    packageid = str(uuid_obj).upper()\n",
    "\n",
    "    # Build single-row DataFrame\n",
    "    data = spark.createDataFrame(\n",
    "        [\n",
    "            (\n",
    "                runcycleid,\n",
    "                None,  # runcyclestartat (will be filled below)\n",
    "                description,\n",
    "                packageid,\n",
    "                packagename,\n",
    "                None,  # runcycleendat\n",
    "                \"NULL\"  # success\n",
    "            )\n",
    "        ],\n",
    "        schema = StructType([\n",
    "                StructField(\"runcycleid\", IntegerType(), False),         # int\n",
    "                StructField(\"runcyclestartat\", TimestampType(), True),   # timestamp\n",
    "                StructField(\"description\", StringType(), True),          # string\n",
    "                StructField(\"packageid\", StringType(), True),            # string\n",
    "                StructField(\"packagename\", StringType(), True),          # string\n",
    "                StructField(\"runcycleendat\", StringType(), True),        # string (you may want to make this a TimestampType too)\n",
    "                StructField(\"success\", StringType(), True),              # string\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Set current timestamp as runcyclestartat\n",
    "    df_with_timestamp = data.withColumn(\"runcyclestartat\", current_timestamp())\n",
    "\n",
    "    # Append to table\n",
    "    df_with_timestamp.write.mode(\"append\").saveAsTable(full_table)\n",
    "\n",
    "    print(f\"✅ Run cycle '{runcycleid}' inserted into {full_table}.\")\n",
    "    return runcycleid\n",
    "\n",
    "def end_run_cycle(\n",
    "    runcycleid: str,\n",
    "    success: str,\n",
    "    packagename: str,\n",
    "    error: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the run cycle row to mark the end of the run.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name: name of the target Delta table\n",
    "    - runcycleid: ID of the run cycle to update\n",
    "    - success: True or False indicating run success\n",
    "    - database: optional database name (default: 'default')\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    full_table = f\"default.dimruncycle\"\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, full_table)\n",
    "\n",
    "    if success == 't':\n",
    "        description = \"package: \" + packagename + \" complete\"\n",
    "    else:        \n",
    "        description = \"package: \" + packagename + \" error \" + error\n",
    "\n",
    "    \n",
    "\n",
    "    # Perform update\n",
    "    delta_table.update(\n",
    "        condition=f\"runcycleid = '{runcycleid}'\",\n",
    "        set={\n",
    "            \"description\": lit(str(description)),\n",
    "            \"runcycleendat\": current_timestamp().cast(\"string\"),\n",
    "            \"success\": lit(str(success).lower()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Run cycle '{runcycleid}' marked as ended with success={success}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "906c5054-19f9-4984-a627-c2747cf435f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# src/utils/job_tracker.py\n",
    "\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "DEFAULT_START_DATE = datetime(2025, 7, 11).date()\n",
    "\n",
    "def _ensure_job_tracker_table_exists(spark: SparkSession, job_tracker_table_path: str):\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hive_metastore.default.job_tracker (\n",
    "            job_name STRING NOT NULL,\n",
    "            run_id STRING NOT NULL,\n",
    "            start_time TIMESTAMP NOT NULL,\n",
    "            end_time TIMESTAMP,\n",
    "            status STRING NOT NULL,\n",
    "            message STRING\n",
    "        ) USING DELTA\n",
    "        LOCATION '{job_tracker_table_path}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.sql(create_table_sql)\n",
    "        print(f\"Ensured job tracker table exists at: {job_tracker_table_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error ensuring job tracker table exists: {e}\")\n",
    "        raise # Re-raise to prevent job from proceeding without tracker\n",
    "\n",
    "\n",
    "def get_last_successful_run_time(spark: SparkSession, job_tracker_table_path: str, job_name: str) -> datetime | None:\n",
    "    try:\n",
    "        _ensure_job_tracker_table_exists(spark, job_tracker_table_path)\n",
    "        tracker_df = spark.read.format(\"delta\").load(job_tracker_table_path)\n",
    "\n",
    "        last_run_df = tracker_df.filter(\n",
    "            (col(\"job_name\") == job_name) & (col(\"status\") == \"SUCCEEDED\")\n",
    "        ).orderBy(col(\"start_time\").desc())\n",
    "\n",
    "        if last_run_df.count() > 0:\n",
    "            last_successful_time = last_run_df.first()[\"start_time\"]\n",
    "            print(f\"Found last successful run for '{job_name}' at: {last_successful_time}\")\n",
    "            return last_successful_time\n",
    "        else:\n",
    "            print(f\"No previous successful run found for '{job_name}'.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading job tracker for last successful run for '{job_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def record_job_status(spark: SparkSession, job_tracker_table_path: str, job_name: str, run_id: str, status: str,\n",
    "                      start_time: datetime, end_time: datetime = None,\n",
    "                      message: str = None):\n",
    "    _ensure_job_tracker_table_exists(spark, job_tracker_table_path) # Ensure table before writing\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"job_name\", StringType(), False),\n",
    "        StructField(\"run_id\", StringType(), False),\n",
    "        StructField(\"start_time\", TimestampType(), False),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"message\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [(job_name, run_id, start_time, end_time, status, message)]\n",
    "\n",
    "    new_status_df = spark.createDataFrame(data, schema=schema)\n",
    "    new_status_df.createOrReplaceTempView(\"new_status_df_temp_view\") # Create a temp view for MERGE\n",
    "\n",
    "    try:\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO delta.`{job_tracker_table_path}` AS target\n",
    "            USING new_status_df_temp_view AS source\n",
    "            ON target.job_name = source.job_name AND target.run_id = source.run_id\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET\n",
    "                    end_time = source.end_time,\n",
    "                    status = source.status,\n",
    "                    message = source.message\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (job_name, run_id, start_time, end_time, status, message)\n",
    "                VALUES (source.job_name, source.run_id, source.start_time, source.end_time, source.status, source.message)\n",
    "        \"\"\"\n",
    "        spark.sql(merge_sql)\n",
    "        print(f\"Job status for '{job_name}' (run_id: {run_id}) recorded as: {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to record job status for '{job_name}' (run_id: {run_id}): {e}\")\n",
    "        raise # Re-raise to ensure the job failure is propagated\n",
    "\n",
    "def get_current_run_id(spark: SparkSession) -> str:\n",
    "    \"\"\"\n",
    "    Get the Databricks run ID from Spark conf, otherwise generates a UUID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.conf.get(\"spark.databricks.driver.runId\")\n",
    "    except Exception:\n",
    "        run_id = str(uuid.uuid4())\n",
    "        print(f\"Warning: spark.databricks.driver.runId not found. Using generated UUID: {run_id}\")\n",
    "        return run_id\n",
    "\n",
    "def generate_date_range_json(last_successful_run_date: datetime | None, current_job_date: datetime) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a JSON array of dates (YYYY-MM-DD) between the last successful run date and the current job date.\n",
    "    \"\"\"\n",
    "    date_list = []\n",
    "    \n",
    "    # Ensure it's date only, not time\n",
    "    end_date = current_job_date.date()\n",
    "\n",
    "    if last_successful_run_date:\n",
    "        # start from a day before the last successful run date\n",
    "        start_date = last_successful_run_date.date()\n",
    "        # Ensure start_date is not after end_date\n",
    "        if start_date > end_date:\n",
    "            start_date = end_date\n",
    "    else:\n",
    "        # If no last successful run, start from DEFAULT_START_DATE.\n",
    "        start_date = DEFAULT_START_DATE\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5066fd-ef21-4d97-8d9d-e45749e4fa57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n",
    "%pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.insert(0, '/Workspace/Shared')\n",
    "# import etl_helpers\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "tablename = \"factmodrequestdocumentdetails\"\n",
    "runcycleid = start_run_cycle(tablename)\n",
    "\n",
    "os.makedirs(\"/dbfs/foi/dataload\", exist_ok=True)  # make sure directory exists\n",
    "\n",
    "try:\n",
    "    df_lastrun = spark.sql(f\"SELECT runcyclestartat as createddate FROM dimruncycle WHERE packagename = \\\"{tablename}\\\" AND success = 't' ORDER BY runcycleid DESC LIMIT 1\")\n",
    "    \n",
    "    # if df_lastrun.count() > 0:\n",
    "    #     lastruntime = df_lastrun.first().createddate.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # else:\n",
    "    #     lastruntime = \"2019-01-01 00:00:00\"\n",
    "\n",
    "    lastruntime = \"2019-01-01 00:00:00\"\n",
    "    endtime = \"2026-03-01 00:00:00\"\n",
    "    print(lastruntime)\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            DISTINCT d.documentid,\n",
    "            d.pagecount,\n",
    "            d.originalpagecount,\n",
    "            d.filename,\n",
    "            dm.ministryrequestid,\n",
    "            dm.documentmasterid,\n",
    "            dm.filepath,\n",
    "            dh.rank1hash,\n",
    "            dh.rank2hash,\n",
    "            CASE\n",
    "                WHEN dd.deleted is not NULL THEN 'Y'\n",
    "            ELSE\n",
    "                'N'\n",
    "            END AS deleted,\n",
    "            TRY_CAST(d.created_at AS DATE) AS created_at,\n",
    "            TRY_CAST(d.updated_at AS STRING) AS updated_at,\n",
    "            0 AS runcycleid,\n",
    "            'Y' AS isactive,\n",
    "            CASE\n",
    "                WHEN d.incompatible = 'f' OR d.incompatible = 'false' THEN 'Y'\n",
    "            ELSE\n",
    "                'N'\n",
    "            END AS compatible,\n",
    "            'FOIMOD' AS sourceoftruth,\n",
    "            mr.foirequestid\n",
    "        FROM docreviewer.Documents d\n",
    "        JOIN docreviewer.DocumentMaster dm ON dm.documentmasterid = d.documentmasterid\n",
    "        JOIN docreviewer.DocumentHashCodes dh ON d.documentid = dh.documentid\n",
    "        LEFT JOIN docreviewer.DocumentDeleted dd ON dm.ministryrequestid = dd.ministryrequestid AND dm.filepath LIKE CONCAT(dd.filepath, '%')\n",
    "        LEFT JOIN (\n",
    "            SELECT\n",
    "                foiministryrequestid,\n",
    "                foirequest_id AS foirequestid\n",
    "            FROM foi_mod.foiministryrequests\n",
    "            QUALIFY ROW_NUMBER() OVER (PARTITION BY foiministryrequestid ORDER BY version DESC) = 1\n",
    "        ) mr on dm.ministryrequestid = mr.foiministryrequestid\n",
    "        WHERE ( d.created_at > TRY_CAST('{lastruntime}' AS TIMESTAMP) and d.created_at <= TRY_CAST('{endtime}' AS TIMESTAMP) )\n",
    "        or ( TRY_CAST(d.updated_at AS TIMESTAMP) > TRY_CAST('{lastruntime}' AS TIMESTAMP) and TRY_CAST(d.updated_at AS TIMESTAMP) <= TRY_CAST('{endtime}' AS TIMESTAMP) )\n",
    "        \"\"\"\n",
    "\n",
    "    print(query)\n",
    "\n",
    "    df = spark.sql(query)\n",
    "    df.show()\n",
    "\n",
    "    # if (df.count() == 0):\n",
    "    #     raise Exception(\"no changes for today\")\n",
    "\n",
    "    # documentids = df.agg(F.concat_ws(\",\", F.collect_list(\"documentid\"))).first()[0]\n",
    "    # print(documentids)\n",
    "\n",
    "    # result_df = spark.sql(\"\"\"\n",
    "    #     update factmodrequestdocumentdetails\n",
    "    #     set isactive = 'N' \n",
    "    #     where documentid in (\"\"\" + documentids + \"\"\")\n",
    "    # \"\"\")\n",
    "\n",
    "    # order of columns here is important!\n",
    "    df_mapped = df.selectExpr(\n",
    "        \"documentid AS documentid\",\n",
    "        \"pagecount AS pagecount\",\n",
    "        \"originalpagecount AS originalpagecount\",\n",
    "        \"filename AS filename\",\n",
    "        \"ministryrequestid AS foiministryrequestid\",\n",
    "        \"documentmasterid AS documentmasterid\",\n",
    "        \"filepath AS filepath\",\n",
    "        \"rank1hash AS rank1hash\",\n",
    "        \"rank2hash AS rank2hash\",\n",
    "        \"deleted AS deleted\",\n",
    "        \"created_at AS created_at\",\n",
    "        \"updated_at AS updated_at\",\n",
    "        f\"{runcycleid} as runcycleid\",\n",
    "        \"'Y' as isactive\",\n",
    "        \"compatible AS compatible\",\n",
    "        \"sourceoftruth AS sourceoftruth\",\n",
    "        \"foirequestid AS foirequestid\"\n",
    "    )\n",
    "    df_mapped.show()\n",
    "    # df_mapped.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"false\").insertInto(tablename)  \n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "    delta_table = DeltaTable.forName(spark, f\"hive_metastore.default.{tablename}\")\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        df_mapped.alias(\"source\"),\n",
    "        \"target.documentid = source.documentid\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition = \"target.isactive = 'Y'\",\n",
    "        set = {\n",
    "            \"isactive\": lit(\"N\"),\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    print(\"Matched records deactivated.\")\n",
    "\n",
    "    df_mapped.write.format(\"delta\").mode(\"append\").saveAsTable(f\"hive_metastore.default.{tablename}\") \n",
    "\n",
    "    end_run_cycle(runcycleid, 't', tablename)\n",
    "except NoCredentialsError:\n",
    "    print(\"Credentials not available\")\n",
    "    end_run_cycle(runcycleid, 'f', tablename, \"Credentials not available\")\n",
    "    raise Exception(\"notebook failed\") from e\n",
    "except Exception as e:\n",
    "    if (str(e) == \"no changes for today\"):\n",
    "        print(\"here\")\n",
    "        end_run_cycle(runcycleid, 't', tablename)\n",
    "    else:\n",
    "        print(f\"An error occurred: {e}\")    \n",
    "        end_run_cycle(runcycleid, 'f', tablename, f\"An error occurred: {e}\")\n",
    "        raise Exception(\"notebook failed\") from e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "factMODRequestDocumentDetailsETL_manually_run",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
